# db.py - init database l·∫ßn ƒë·∫ßu
import os
import json
import time
import psycopg2
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path

import psycopg2
from urllib.parse import urlparse

def get_db_connection():
    load_dotenv()
    DATABASE_URL = os.getenv("DATABASE_URL")
    
    if DATABASE_URL:
        # Parse URL chu·∫©n
        parsed = urlparse(DATABASE_URL)
        conn_params = {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'dbname': parsed.path[1:],
            'user': parsed.username,
            'password': parsed.password
        }
        log_step("‚úÖ Parsed DB", f"{conn_params['host']}:{conn_params['port']}/{conn_params['dbname']}")
        return psycopg2.connect(**conn_params)
    else:
        return psycopg2.connect(
            host='localhost', port=5432, dbname='bidding_data', 
            user='postgres', password=''
        )

def log_step(step, details=""):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {step} {details}")

def clean_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df = df.replace([np.nan, np.inf, -np.inf], None)
    for col in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            df[col] = pd.to_datetime(df[col], errors="coerce")
    return df

def df_to_records(df: pd.DataFrame):
    records = []
    cols = df.columns.tolist()
    for _, row in df.iterrows():
        records.append(tuple(row[c] for c in cols))
    return records, cols

def insert_chunk(cur, table_name, columns, records, chunk_size=1000):
    if not records:
        return
    cols_str = ", ".join(f'"{c}"' for c in columns)
    placeholders = ", ".join(["%s"] * len(columns))
    sql = f'INSERT INTO {table_name} ({cols_str}) VALUES ({placeholders})'
    for i in range(0, len(records), chunk_size):
        chunk = records[i:i+chunk_size]
        cur.executemany(sql, chunk)
        log_step(f"üì§ Insert {table_name}", f"chunk {i//chunk_size+1}: {len(chunk)} rows")

def main():
    start = time.time()
    log_step("üöÄ DB INIT START", "="*40)

    # 1. Load files
    log_step("üìÇ Loading Excel/JSON...")
    df1 = clean_df(pd.read_excel("processed/columns_19_20.xlsx"))
    df2 = clean_df(pd.read_excel("processed/columns_13_14.xlsx"))
    add_info = clean_df(pd.read_excel("processed/additional_info_log.xlsx"))

    run_history_file = Path("processed/run_history.json")
    if run_history_file.exists():
        with open(run_history_file, "r", encoding="utf-8") as f:
            run_history_data = json.load(f)
    else:
        run_history_data = []

    log_step("‚úÖ Loaded",
             f"df1={len(df1)}, df2={len(df2)}, add_info={len(add_info)}, runs={len(run_history_data)}")

    conn = None
    try:
        log_step("üîó Connecting PostgreSQL...")
        conn = get_db_connection()
        cur = conn.cursor()

        # 2. Drop & create tables
        log_step("‚öôÔ∏è Creating tables...")
        cur.execute("DROP TABLE IF EXISTS df1_standard CASCADE;")
        cur.execute("DROP TABLE IF EXISTS df2_extended CASCADE;")
        cur.execute("DROP TABLE IF EXISTS additional_info_log CASCADE;")
        cur.execute("DROP TABLE IF EXISTS run_history CASCADE;")

        cur.execute("""
        CREATE TABLE df1_standard (
                id SERIAL PRIMARY KEY,
                "M√£ TBMT" TEXT,
                "T√™n thu·ªëc" TEXT,
                "T√™n ho·∫°t ch·∫•t" TEXT,
                "N·ªìng ƒë·ªô, h√†m l∆∞·ª£ng" TEXT,
                "ƒê∆∞·ªùng d√πng" TEXT,
                "D·∫°ng b√†o ch·∫ø" TEXT,
                "Quy c√°ch" TEXT,
                "Nh√≥m thu·ªëc" TEXT,
                "GƒêKLH ho·∫∑c GPNK" TEXT,
                "C∆° s·ªü s·∫£n xu·∫•t" TEXT,
                "Xu·∫•t x·ª©" TEXT,
                "ƒê∆°n v·ªã t√≠nh" TEXT,
                "S·ªë l∆∞·ª£ng" NUMERIC,
                "ƒê∆°n gi√° tr√∫ng th·∫ßu (VND)" NUMERIC,
                "Th√†nh ti·ªÅn (VND)" NUMERIC,
                "Nh√† th·∫ßu tr√∫ng th·∫ßu" TEXT,
                "H·∫°n d√πng (tu·ªïi th·ªç)" TEXT,
                created_at TIMESTAMP DEFAULT NOW()
        );
        """)

        cur.execute("""
        CREATE TABLE df2_extended (
                id SERIAL PRIMARY KEY,
                "M√£ TBMT" TEXT,
                "T√™n h√†ng h√≥a" TEXT,
                "Nh√£n hi·ªáu" TEXT,
                "K√Ω m√£ hi·ªáu" TEXT,
                "T√≠nh nƒÉng k·ªπ thu·∫≠t" TEXT,
                "Xu·∫•t x·ª©" TEXT,
                "H√£ng s·∫£n xu·∫•t" TEXT,
                "ƒê∆°n v·ªã t√≠nh" TEXT,
                "Kh·ªëi l∆∞·ª£ng" NUMERIC,
                "ƒê∆°n gi√° tr√∫ng th·∫ßu (VND)" NUMERIC,
                "Th√†nh ti·ªÅn (VND)" NUMERIC,
                "Nh√† th·∫ßu tr√∫ng th·∫ßu" TEXT,
                "search" TEXT,
                created_at TIMESTAMP DEFAULT NOW()
        );
        """)

        cur.execute("""
        CREATE TABLE additional_info_log (
            id SERIAL PRIMARY KEY,
            "M√£ TBMT" TEXT,
            "Ch·ªß ƒë·∫ßu t∆∞" TEXT,
            "Quy·∫øt ƒë·ªãnh ph√™ duy·ªát" TEXT,
            "Ng√†y ph√™ duy·ªát" DATE,
            "Ng√†y h·∫øt hi·ªáu l·ª±c" DATE,
            "ƒê·ªãa ƒëi·ªÉm" TEXT,
            "H√¨nh th·ª©c LCNT" TEXT,
            "T√¨nh tr·∫°ng hi·ªáu l·ª±c" TEXT,
            created_at TIMESTAMP DEFAULT NOW()
        );
        """)

        cur.execute("""
        CREATE TABLE run_history (
            id SERIAL PRIMARY KEY,
            start_time TIMESTAMP,
            end_time TIMESTAMP,
            duration_seconds INTEGER,
            boxes_selected INTEGER,
            created_at TIMESTAMP DEFAULT NOW()
        );
        """)

        log_step("‚úÖ Tables created")

        # 3. Insert df1, df2, add_info
        if len(df1) > 0:
            rec1, cols1 = df_to_records(df1)
            insert_chunk(cur, "df1_standard", cols1, rec1)

        if len(df2) > 0:
            rec2, cols2 = df_to_records(df2)
            insert_chunk(cur, "df2_extended", cols2, rec2)

        if len(add_info) > 0:
            rec3, cols3 = df_to_records(add_info)
            insert_chunk(cur, "additional_info_log", cols3, rec3)

        # 4. Insert run_history
        if run_history_data:
            log_step("üì§ Inserting run_history...", f"{len(run_history_data)} rows")
            sql = """
                INSERT INTO run_history (start_time, end_time, duration_seconds, boxes_selected)
                VALUES (%s, %s, %s, %s)
            """
            rows = []
            for item in run_history_data:
                rows.append((
                    item.get("start_time"),
                    item.get("end_time"),
                    item.get("duration_seconds"),
                    item.get("boxes_selected"),
                ))
            cur.executemany(sql, rows)

        # 5. Indexes
        log_step("‚öôÔ∏è Creating VIEWS...")
    
        cur.execute("DROP VIEW IF EXISTS df1_full CASCADE;")
        cur.execute("DROP VIEW IF EXISTS df2_full CASCADE;")
        
        cur.execute("""
            CREATE OR REPLACE VIEW df1_full AS
            SELECT 
                d1.*,
                ai."Ch·ªß ƒë·∫ßu t∆∞",
                ai."Quy·∫øt ƒë·ªãnh ph√™ duy·ªát",
                ai."Ng√†y ph√™ duy·ªát",
                ai."Ng√†y h·∫øt hi·ªáu l·ª±c",
                ai."ƒê·ªãa ƒëi·ªÉm",
                ai."H√¨nh th·ª©c LCNT",
                ai."T√¨nh tr·∫°ng hi·ªáu l·ª±c"
            FROM df1_standard d1
            LEFT JOIN additional_info_log ai ON ai."M√£ TBMT" = d1."M√£ TBMT"
        """)
        
        cur.execute("""
            CREATE OR REPLACE VIEW df2_full AS
            SELECT 
                d2.*,
                ai."Ch·ªß ƒë·∫ßu t∆∞",
                ai."Quy·∫øt ƒë·ªãnh ph√™ duy·ªát",
                ai."Ng√†y ph√™ duy·ªát",
                ai."Ng√†y h·∫øt hi·ªáu l·ª±c", 
                ai."ƒê·ªãa ƒëi·ªÉm",
                ai."H√¨nh th·ª©c LCNT",
                ai."T√¨nh tr·∫°ng hi·ªáu l·ª±c"
            FROM df2_extended d2
            LEFT JOIN additional_info_log ai ON ai."M√£ TBMT" = d2."M√£ TBMT"
        """)
        
        conn.commit()
        log_step("‚úÖ VIEWS created")
        
        log_step("‚öôÔ∏è Creating INDEXES for base tables...")
        indexes = [
            'CREATE INDEX idx_df1_ma_tbmt ON df1_standard("M√£ TBMT");',
            'CREATE INDEX idx_df1_donvitinh ON df1_standard("ƒê∆°n v·ªã t√≠nh");',
            'CREATE INDEX idx_df1_soluong ON df1_standard("S·ªë l∆∞·ª£ng");',
            'CREATE INDEX idx_df1_dongia ON df1_standard("ƒê∆°n gi√° tr√∫ng th·∫ßu (VND)");',
            'CREATE INDEX idx_df1_thanhtien ON df1_standard("Th√†nh ti·ªÅn (VND)");',
            'CREATE INDEX idx_df1_tenthuoc ON df1_standard("T√™n thu·ªëc");', 
            'CREATE INDEX idx_df1_xuat_xu ON df1_standard("Xu·∫•t x·ª©");',
            'CREATE INDEX idx_df1_nhathau ON df1_standard("Nh√† th·∫ßu tr√∫ng th·∫ßu");',

            'CREATE INDEX idx_df2_ma_tbmt ON df2_extended("M√£ TBMT");',
            'CREATE INDEX idx_df2_donvitinh ON df2_extended("ƒê∆°n v·ªã t√≠nh");',
            'CREATE INDEX idx_df2_soluong ON df2_extended("Kh·ªëi l∆∞·ª£ng");',
            'CREATE INDEX idx_df2_dongia ON df2_extended("ƒê∆°n gi√° tr√∫ng th·∫ßu (VND)");',
            'CREATE INDEX idx_df2_thanhtien ON df2_extended("Th√†nh ti·ªÅn (VND)");',
            'CREATE INDEX idx_df2_ten_hang_hoa ON df2_extended("T√™n h√†ng h√≥a");',
            'CREATE INDEX idx_df2_xuat_xu ON df2_extended("Xu·∫•t x·ª©");',
            'CREATE INDEX idx_df2_nhathau ON df2_extended("Nh√† th·∫ßu tr√∫ng th·∫ßu");',

            'CREATE INDEX idx_ai_ma_tbmt ON additional_info_log("M√£ TBMT");',
            'CREATE INDEX idx_ai_chu_dau_tu ON additional_info_log("Ch·ªß ƒë·∫ßu t∆∞");',
            'CREATE INDEX idx_ai_quyet_dinh ON additional_info_log("Quy·∫øt ƒë·ªãnh ph√™ duy·ªát");',
            'CREATE INDEX idx_ai_ngay_phe_duyet ON additional_info_log("Ng√†y ph√™ duy·ªát");',
            'CREATE INDEX idx_ai_ngay_het_hieu_luc ON additional_info_log("Ng√†y h·∫øt hi·ªáu l·ª±c");',
            'CREATE INDEX idx_ai_dia_diem ON additional_info_log("ƒê·ªãa ƒëi·ªÉm");',
            'CREATE INDEX idx_ai_tinh_trang ON additional_info_log("T√¨nh tr·∫°ng hi·ªáu l·ª±c");',
        ]
        
        for idx_sql in indexes:
            cur.execute(idx_sql)
        
        log_step("‚úÖ INDEXES created")

        # Verify
        for view_name in ["df1_full", "df2_full"]:
            cur.execute(f"SELECT COUNT(*) FROM {view_name}")
            cnt = cur.fetchone()[0]
            log_step("üìä VIEW rows", f"{view_name}: {cnt:,} rows")

        log_step("üéâ DB INIT COMPLETED", f"{time.time()-start:.1f}s")

    except Exception as e:
        if conn:
            conn.rollback()
        log_step("‚ùå DB INIT FAILED", str(e))
        import traceback; traceback.print_exc()
    finally:
        if conn:
            conn.close()
            log_step("üîå CONNECTION CLOSED ========================================")

if __name__ == "__main__":
    main()
